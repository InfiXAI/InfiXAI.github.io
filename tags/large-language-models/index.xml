<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Large Language Models on InfiX-AI</title>
    <link>http://infixai.github.io/tags/large-language-models/</link>
    <description>Recent content in Large Language Models on InfiX-AI</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 20 May 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://infixai.github.io/tags/large-language-models/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models</title>
      <link>http://infixai.github.io/research/infifpo/</link>
      <pubDate>Tue, 20 May 2025 00:00:00 +0000</pubDate>
      <guid>http://infixai.github.io/research/infifpo/</guid>
      <description>&lt;p&gt;We propose &lt;strong&gt;InfiFPO&lt;/strong&gt;, a principled and efficient framework for performing model fusion during the preference alignment phase. Our key insight is that the reference model in preference optimization (e.g., in DPO) can be replaced with a fused source model, thereby enabling the pivot model to learn not only from preference data but also from the probabilistic behaviors of multiple source models.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;assets/exp.png&#34; alt=&#34;InfiFPO&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Comprehensive experiments on 11 widely-used benchmarks demonstrate that &lt;strong&gt;InfiFPO&lt;/strong&gt; consistently outperforms existing model fusion and preference optimization methods. When using Phi-4 as the pivot model, &lt;strong&gt;InfiFPO&lt;/strong&gt; improve its average performance from 79.95 to 83.33 on 11 benchmarks, significantly improving its capabilities in mathematics, coding, and reasoning tasks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion</title>
      <link>http://infixai.github.io/research/infigfusion/</link>
      <pubDate>Tue, 20 May 2025 00:00:00 +0000</pubDate>
      <guid>http://infixai.github.io/research/infigfusion/</guid>
      <description>&lt;p&gt;&lt;strong&gt;InfiGFusion&lt;/strong&gt; is the first structure-aware fusion framework for large language models that models semantic dependencies among logits using feature-level graphs. We introduce a novel Graph-on-Logits Distillation (GLD) loss that captures cross-dimension interactions via co-activation graphs and aligns them using an efficient, provable approximation of Gromov-Wasserstein distance (reducing complexity from O(n^4) to O(nlogn)). Our released &lt;strong&gt;InfiGFusion-14B&lt;/strong&gt; model consistently shows better performance, achieving +35.6 on Multistep Arithmetic and +37.06 on Causal Judgement over SFT, demonstrating superior multi-step and complex logic inference.&lt;/p&gt;</description>
    </item>
    <item>
      <title>InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via LLM Fusion</title>
      <link>http://infixai.github.io/research/infifusion/</link>
      <pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://infixai.github.io/research/infifusion/</guid>
      <description>&lt;p&gt;&lt;strong&gt;InfiFusion&lt;/strong&gt; is the first fusion framework for large language models that fuse up to 4 models with 14B~24B parameters. We introduce a unified framework which can fuse many heterogeneous models&#xA;in one distillation stage. InfiFusion outperforms the state-of-the-art models, such as Qwen-2.5-14B-Instruct and Phi-4, across 11 widely applied benchmarks covering reasoning, coding, mathematics, and instruction-following tasks. Notably, InfiFusion achieves this superior performance while significantly reduces&#xA;computational costs, completing full training with only 160 H800 GPU hours compared to the millions typically required for traditional LLM training.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
